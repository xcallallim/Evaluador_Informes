"""Entrenador para la puntuación de preguntas institucionales."""

# python -m training.institutional_trainer

from __future__ import annotations

import argparse
import json
import math
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Iterable

import joblib
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV, KFold, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MaxAbsScaler, OneHotEncoder

from utils.training_export import TrainingDataset, load_latest_training_dataset


TARGET_SCORE_COLUMN = "score"
DEFAULT_PATTERN = "dataset_entrenamiento_ml*.xlsx"
DEFAULT_DIRECTORY = Path("data/examples")
DEFAULT_OUTPUT_DIR = Path("models")


@dataclass(slots=True)
class TrainingArtifacts:
    """Persisted objects generated by the training routine."""

    model_path: Path
    metrics_path: Path


@dataclass(slots=True)
class EvaluationReport:
    """Container for the evaluation artefacts."""

    global_metrics: dict[str, float]
    per_question: list[dict[str, object]]
    per_criterion: list[dict[str, object]]
    best_params: dict[str, object]
    cv_score: float


_TEXT_CLEANER = re.compile(r"\s+")


def clean_text(value: object) -> str:
    """Normalise whitespace, strip and lowercase ``value``."""

    if not isinstance(value, str):
        return ""
    lowered = value.lower()
    collapsed = _TEXT_CLEANER.sub(" ", lowered)
    return collapsed.strip()


def prepare_institutional_dataset(dataset: TrainingDataset) -> pd.DataFrame:
    """Filter the dataset to institutional entries and build helper columns."""

    dataframe = dataset.dataframe.copy()
    institutional_mask = (
        dataframe["tipo_informe"].astype(str).str.lower().str.strip() == "institucional"
    )
    institutional = dataframe.loc[institutional_mask].copy()
    if institutional.empty:
        raise ValueError("No se encontraron registros de tipo institucional en el dataset.")

    needs_text_fallback = institutional["texto"].isna().all() or institutional["texto"].astype(str).str.strip().eq("").all()
    if needs_text_fallback:
        raw_preguntas = pd.read_excel(dataset.path, sheet_name="preguntas")
        raw_candidates = raw_preguntas.loc[institutional_mask]
        for column in ("texto", "relevant_text"):
            if column in raw_candidates:
                candidate_series = raw_candidates[column].fillna("")
                if candidate_series.astype(str).str.strip().eq("").all():
                    continue
                institutional["texto"] = candidate_series.values
                break
    institutional["texto"] = institutional["texto"].fillna("")
    institutional["clean_text"] = institutional["texto"].map(clean_text)
    institutional["seccion"] = institutional["seccion"].fillna("desconocido")
    institutional["criterio"] = institutional["criterio"].fillna("desconocido")
    institutional["pregunta"] = institutional["pregunta"].fillna("desconocido")

    # Simple metadata features
    institutional["text_length"] = institutional["clean_text"].str.len().astype(float)
    institutional["word_count"] = (
        institutional["clean_text"].str.split().map(len).astype(float)
    )

    institutional = institutional[
        [
            "clean_text",
            "seccion",
            "criterio",
            "pregunta",
            "text_length",
            "word_count",
            TARGET_SCORE_COLUMN,
        ]
    ].reset_index(drop=True)

    return institutional


def build_preprocessor() -> ColumnTransformer:
    """Create the feature extraction pipeline."""

    text_vectorizer = TfidfVectorizer(
        strip_accents="unicode",
        ngram_range=(1, 2),
    )

    metadata_encoder = OneHotEncoder(handle_unknown="ignore")
    numeric_pipeline = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="constant", fill_value=0.0)),
            ("scaler", MaxAbsScaler()),
        ]
    )

    return ColumnTransformer(
        transformers=[
            ("text", text_vectorizer, "clean_text"),
            ("metadata", metadata_encoder, ["seccion", "criterio"]),
            ("length", numeric_pipeline, ["text_length", "word_count"]),
        ]
    )


def build_model_pipeline() -> Pipeline:
    """Combine preprocessing and regression into a single pipeline."""

    preprocessor = build_preprocessor()
    regressor = Ridge(solver="lsqr")

    return Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("regressor", regressor),
        ]
    )


def _clip_predictions(predictions: np.ndarray) -> np.ndarray:
    return np.clip(predictions, 0.0, 4.0)


def train_model(
    dataframe: pd.DataFrame,
    *,
    test_size: float = 0.2,
    random_state: int = 42,
    n_splits: int = 5,
) -> tuple[Pipeline, EvaluationReport, pd.DataFrame]:
    """Train the Ridge regressor with a light grid search and report metrics."""

    features = dataframe.drop(columns=[TARGET_SCORE_COLUMN])
    target = dataframe[TARGET_SCORE_COLUMN].astype(float)

    X_train, X_test, y_train, y_test = train_test_split(
        features,
        target,
        test_size=test_size,
        random_state=random_state,
    )

    pipeline = build_model_pipeline()

    param_grid = {
        "preprocess__text__ngram_range": [(1, 1), (1, 2)],
        "regressor__alpha": [0.1, 1.0, 10.0],
    }

    cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    search = GridSearchCV(
        pipeline,
        param_grid=param_grid,
        scoring="neg_mean_absolute_error",
        cv=cv,
        n_jobs=-1,
    )
    search.fit(X_train, y_train)

    best_model: Pipeline = search.best_estimator_

    predictions = best_model.predict(X_test)
    clipped_predictions = _clip_predictions(predictions)

    evaluation_frame = X_test.copy()
    evaluation_frame[TARGET_SCORE_COLUMN] = y_test
    evaluation_frame["prediction"] = clipped_predictions
    evaluation_frame["abs_error"] = (evaluation_frame[TARGET_SCORE_COLUMN] - clipped_predictions).abs()
    evaluation_frame["squared_error"] = (
        (evaluation_frame[TARGET_SCORE_COLUMN] - clipped_predictions) ** 2
    )

    global_metrics = {
        "mae": float(mean_absolute_error(y_test, clipped_predictions)),
        "rmse": float(mean_squared_error(y_test, clipped_predictions, squared=False)),
        "r2": float(r2_score(y_test, clipped_predictions)),
    }

    def _group_metrics(grouped: Iterable[tuple[str, pd.DataFrame]]) -> list[dict[str, object]]:
        records: list[dict[str, object]] = []
        for group_name, group_df in grouped:
            mae = float(group_df["abs_error"].mean())
            rmse = float(math.sqrt(group_df["squared_error"].mean()))
            records.append(
                {
                    "name": group_name,
                    "count": int(group_df.shape[0]),
                    "mae": mae,
                    "rmse": rmse,
                    "true_mean": float(group_df[TARGET_SCORE_COLUMN].mean()),
                    "pred_mean": float(group_df["prediction"].mean()),
                    "true_min": float(group_df[TARGET_SCORE_COLUMN].min()),
                    "true_max": float(group_df[TARGET_SCORE_COLUMN].max()),
                    "pred_min": float(group_df["prediction"].min()),
                    "pred_max": float(group_df["prediction"].max()),
                }
            )
        return records

    per_question = _group_metrics(evaluation_frame.groupby("pregunta"))
    per_criterion = _group_metrics(evaluation_frame.groupby("criterio"))

    report = EvaluationReport(
        global_metrics=global_metrics,
        per_question=per_question,
        per_criterion=per_criterion,
        best_params={str(key): value for key, value in search.best_params_.items()},
        cv_score=float(-search.best_score_),
    )

    return best_model, report, evaluation_frame


def persist_artifacts(model: Pipeline, report: EvaluationReport, output_dir: Path) -> TrainingArtifacts:
    """Serialise the trained pipeline and metrics summary."""

    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    model_path = output_dir / f"institutional_ridge_{timestamp}.joblib"
    metrics_path = output_dir / f"institutional_ridge_{timestamp}_metrics.json"

    joblib.dump(model, model_path)

    payload = {
        "global_metrics": report.global_metrics,
        "per_question": report.per_question,
        "per_criterion": report.per_criterion,
        "best_params": report.best_params,
        "cv_score": report.cv_score,
        "generated_at": timestamp,
    }

    with metrics_path.open("w", encoding="utf-8") as stream:
        json.dump(payload, stream, indent=2, ensure_ascii=False)

    return TrainingArtifacts(model_path=model_path, metrics_path=metrics_path)


def run_training(pattern: str, directory: Path, output_dir: Path) -> TrainingArtifacts:
    dataset = load_latest_training_dataset(pattern=pattern, directory=directory)
    institutional = prepare_institutional_dataset(dataset)
    model, report, _ = train_model(institutional)

    artifacts = persist_artifacts(model, report, output_dir)

    print("Resumen global:")
    for metric, value in report.global_metrics.items():
        print(f"  {metric}: {value:.4f}")

    print("\nMejores hiperparámetros:")
    for param, value in report.best_params.items():
        print(f"  {param}: {value}")

    print(f"\nPuntaje CV (MAE): {report.cv_score:.4f}")
    print(f"Modelo guardado en: {artifacts.model_path}")
    print(f"Reporte guardado en: {artifacts.metrics_path}")

    return artifacts


def build_argument_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Entrena un modelo Ridge sencillo para puntuar preguntas institucionales.",
    )
    parser.add_argument(
        "--pattern",
        default=DEFAULT_PATTERN,
        help="Patrón glob para localizar el dataset de entrenamiento.",
    )
    parser.add_argument(
        "--directory",
        default=str(DEFAULT_DIRECTORY),
        help="Directorio donde se buscará el dataset.",
    )
    parser.add_argument(
        "--output-dir",
        default=str(DEFAULT_OUTPUT_DIR),
        help="Directorio donde se almacenarán el modelo y métricas.",
    )
    return parser


def main() -> None:
    parser = build_argument_parser()
    args = parser.parse_args()

    directory = Path(args.directory)
    output_dir = Path(args.output_dir)

    run_training(pattern=args.pattern, directory=directory, output_dir=output_dir)


if __name__ == "__main__":
    main()